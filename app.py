import os
import sys

# Force the project root onto the path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from dotenv import load_dotenv
import gradio as gr
import yaml
import json
import re
from chat.llm_functions import get_interviewer_response, get_student_response, generate_cover_letter_response
from utils import parse_json_from_response
from guide_generation.llm_functions import generate_guide as create_guide_from_llm
from answer_flow_generation.llm_functions import generate_answer_flow


# Load environment variables and initial data
load_dotenv()

# with open("prompt.yaml", "r", encoding='utf-8') as f:
#     prompts = yaml.safe_load(f)

with open("example_info.json", "r", encoding='utf-8') as f:
    # This now serves as the default values for the UI
    default_info = json.load(f)
    # word_limit ê¸°ë³¸ê°’ ì¶”ê°€
    if 'word_limit' not in default_info:
        default_info['word_limit'] = 300

def user_submit(message, history):
    """ì‚¬ìš©ì ì…ë ¥ì„ ì²˜ë¦¬í•˜ê³ , ì±—ë´‡ ê¸°ë¡ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    if not message.strip():
        return "", history
    history.append([message, None])
    return "", history

def clean_markdown_response(text):
    """
    LLM ì‘ë‹µì—ì„œ markdown ì½”ë“œ ë¸”ë¡ì„ ì œê±°í•˜ê³  ì‹¤ì œ ë‚´ìš©ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        text (str): LLM ì‘ë‹µ í…ìŠ¤íŠ¸
        
    Returns:
        str: ì •ë¦¬ëœ í…ìŠ¤íŠ¸
    """
    if not text:
        return text
    
    # ```markdown ... ``` ë˜ëŠ” ``` ... ``` íŒ¨í„´ ì œê±°
    import re
    
    # markdown ì½”ë“œ ë¸”ë¡ íŒ¨í„´ ì°¾ê¸°
    markdown_match = re.search(r"```(?:markdown)?\s*([\s\S]*?)\s*```", text)
    if markdown_match:
        return markdown_match.group(1).strip()
    
    # ì¼ë°˜ì ì¸ ì½”ë“œ ë¸”ë¡ íŒ¨í„´ ì°¾ê¸°
    code_match = re.search(r"```\s*([\s\S]*?)\s*```", text)
    if code_match:
        return code_match.group(1).strip()
    
    # ì½”ë“œ ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
    return text.strip()

def bot_response(history, shared_info, progress=gr.Progress()):
    """ë©´ì ‘ê´€ì˜ ì‘ë‹µì„ ìƒì„±í•˜ê³  ì§„í–‰ë¥ ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    if not history or history[-1][1] is not None:
        return history, gr.update(), gr.update()
    
    conversation_str = ""
    for h in history:
        conversation_str += f"í•™ìƒ: {h[0]}\n"
        if h[1]:
            conversation_str += f"AI: {h[1]}\n"
    
    format_info = shared_info.copy()
    format_info['conversation'] = conversation_str
    # word_limit ê¸°ë³¸ê°’ ì„¤ì • (í˜¹ì‹œ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„)
    if 'word_limit' not in format_info:
        format_info['word_limit'] = 300

    history[-1][1] = ""
    full_response = ""
    for chunk in get_interviewer_response(format_info):
        full_response += chunk
        history[-1][1] = full_response
        yield history, gr.update(), gr.update()

    final_data = parse_json_from_response(full_response)
    final_progress_update = gr.update()
    final_reason_update = gr.update()
    if final_data:
        history[-1][1] = final_data.get("answer", "ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        final_progress = final_data.get("progress", 0)
        reasoning = final_data.get("reasoning_for_progress", "")
        
        if isinstance(final_progress, int) and 0 <= final_progress <= 100:
            progress(final_progress / 100)
            final_progress_update = f"ìê¸°ì†Œê°œì„œ ì™„ì„±ë„: {final_progress}%"
            if reasoning:
                final_reason_update = gr.update(value=f"**ì§„í–‰ ìƒí™© ë¶„ì„:** {reasoning}", visible=True)
            else:
                final_reason_update = gr.update(visible=False)

        if final_progress >= 100:
             history.append([None, "ë©´ì ‘ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìê¸°ì†Œê°œì„œ ìƒì„± íƒ­ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”."])
    
    yield history, final_progress_update, final_reason_update


def generate_ai_reply(history, shared_info, progress=gr.Progress()):
    """í•™ìƒì˜ AI ë‹µë³€ì„ ìƒì„±í•˜ê³ , ê·¸ì— ëŒ€í•œ ë©´ì ‘ê´€ì˜ í›„ì† ì§ˆë¬¸ì„ ë°›ìŠµë‹ˆë‹¤."""
    if not history or not history[-1][1]:
        return history, gr.update(), gr.update()
    
    conversation_str = ""
    for h in history:
        conversation_str += f"í•™ìƒ: {h[0]}\n"
        if h[1]:
            conversation_str += f"AI: {h[1]}\n"

    format_info = shared_info.copy()
    format_info['conversation'] = conversation_str
    # word_limit ê¸°ë³¸ê°’ ì„¤ì • (í˜¹ì‹œ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„)
    if 'word_limit' not in format_info:
        format_info['word_limit'] = 300

    student_answer_json = ""
    history.append(["", None])
    for chunk in get_student_response(format_info):
        student_answer_json += chunk
        parsed_data = parse_json_from_response(student_answer_json)
        if parsed_data:
            history[-1][0] = parsed_data.get("answer", "")
        else:
            history[-1][0] = student_answer_json
        yield history, gr.update(), gr.update()

    final_data = parse_json_from_response(student_answer_json)
    if final_data:
        history[-1][0] = final_data.get("answer", "ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    yield history, gr.update(), gr.update()

    yield from bot_response(history, shared_info, progress=progress)

def generate_all_cover_letters(history, shared_info, progress=gr.Progress()):
    """ëª¨ë“  ìê¸°ì†Œê°œì„œ ë¬¸í•­ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ê³  ì§„í–‰ë¥ ì„ í‘œì‹œí•©ë‹ˆë‹¤."""
    if not history:
        empty_outputs = [gr.update(value="ë©´ì ‘ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")] * len(shared_info.get('questions', []))
        empty_guidelines = [gr.update(value="")] * len(shared_info.get('questions', []))
        return empty_outputs + empty_guidelines + [gr.update()]

    # history -> conversation_history í˜•ì‹ ë³€í™˜
    conversation_str = ""
    for h in history:
        if h[0]: conversation_str += f"í•™ìƒ: {h[0]}\n"
        if h[1]: conversation_str += f"AI: {h[1]}\n"

    total_questions = len(shared_info.get('questions', []))
    outputs = [""] * total_questions
    guidelines = [""] * total_questions
    
    format_info = shared_info.copy()
    format_info['conversation'] = conversation_str
    
    for i, question in enumerate(shared_info.get('questions', [])):
        # 1ë‹¨ê³„: Answer Flow Generation
        progress_text = f"ìê¸°ì†Œê°œì„œ ìƒì„± ì§„í–‰ë¥ : {int((i / total_questions) * 50)}% (ë‹µë³€ íë¦„ ìƒì„± ì¤‘...)"
        yield [gr.update(value=o) for o in outputs] + [gr.update(value=g) for g in guidelines] + [gr.update(value=progress_text, visible=True)]
        
        flow_result, _ = generate_answer_flow(
            question=question,
            jd=format_info.get('jd', ''),
            company_name=format_info.get('company_name', ''),
            experience_level=format_info.get('experience_level', 'ì‹ ì…'),
            conversation=conversation_str
        )
        
        flow_text = flow_result.get('flow', '') if flow_result else ''
        guidelines[i] = flow_text  # ê°€ì´ë“œë¼ì¸ ì €ì¥
        
        # 2ë‹¨ê³„: Cover Letter Response Generation
        progress_text = f"ìê¸°ì†Œê°œì„œ ìƒì„± ì§„í–‰ë¥ : {int((i / total_questions) * 50 + 25)}% (ë‹µë³€ ìƒì„± ì¤‘...)"
        yield [gr.update(value=o) for o in outputs] + [gr.update(value=g) for g in guidelines] + [gr.update(value=progress_text, visible=True)]
        
        full_response = ""
        word_limit = shared_info.get('word_limit', 300)  # shared_infoì—ì„œ word_limit ê°€ì ¸ì˜¤ê¸°
        for chunk in generate_cover_letter_response(question, [], format_info, flow_text, word_limit):
            full_response += chunk
            parsed_data = parse_json_from_response(full_response)
            if parsed_data and 'answer' in parsed_data:
                # JSONì—ì„œ ë‹µë³€ì„ ì¶”ì¶œí•œ í›„ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì •ë¦¬
                cleaned_answer = clean_markdown_response(parsed_data['answer'])
                outputs[i] = cleaned_answer
            else:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì „ì²´ ì‘ë‹µì—ì„œ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì •ë¦¬
                cleaned_response = clean_markdown_response(full_response)
                outputs[i] = cleaned_response
            
            overall_progress_val = (i + 0.75) / total_questions
            progress_text = f"ìê¸°ì†Œê°œì„œ ìƒì„± ì§„í–‰ë¥ : {int(overall_progress_val*100)}%"
            yield [gr.update(value=o) for o in outputs] + [gr.update(value=g) for g in guidelines] + [gr.update(value=progress_text, visible=True)]

        # ìµœì¢… íŒŒì‹± ë° ì •ë¦¬
        final_data = parse_json_from_response(full_response)
        if final_data and 'answer' in final_data:
            # JSONì—ì„œ ë‹µë³€ì„ ì¶”ì¶œí•œ í›„ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì •ë¦¬
            cleaned_answer = clean_markdown_response(final_data['answer'])
            outputs[i] = cleaned_answer
        else:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì „ì²´ ì‘ë‹µì—ì„œ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì •ë¦¬
            cleaned_response = clean_markdown_response(full_response)
            outputs[i] = cleaned_response

    # ì™„ë£Œ
    yield [gr.update(value=o) for o in outputs] + [gr.update(value=g) for g in guidelines] + [gr.update(visible=False)]

def update_guide_and_info(company, position, jd, questions_str, word_limit):
    guide_json, _ = create_guide_from_llm(questions_str, jd, company, "ì‹ ì…") # experience_level is hardcoded for now
    
    if guide_json and "guide" in guide_json:
        guide_text = guide_json["guide"]
    else:
        guide_text = "ê°€ì´ë“œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì…ë ¥ê°’ì„ í™•ì¸í•´ì£¼ì„¸ìš”."

    new_info = default_info.copy()
    new_info.update({
        "company_name": company,
        "position_title": position,
        "jd": jd,
        "questions": [q.strip() for q in questions_str.strip().split('\n') if q.strip()],
        "guide": guide_text,
        "word_limit": word_limit
    })
    
    # Return new state and update for the guide display
    return new_info, guide_text

# --- Gradio UI ---
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    shared_info = gr.State(default_info)

    with gr.Tabs() as tabs:
        with gr.TabItem("ê°€ì´ë“œ ìƒì„±", id=0):
            gr.Markdown("## ğŸ“ ìê¸°ì†Œê°œì„œ ì •ë³´ ì…ë ¥")
            gr.Markdown("ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜ì— í•„ìš”í•œ ì •ë³´ë¥¼ ì…ë ¥í•˜ê³  'ê°€ì´ë“œ ìƒì„±' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
            with gr.Row():
                company_name_input = gr.Textbox(label="íšŒì‚¬ëª…", value=default_info.get("company_name"))
                position_title_input = gr.Textbox(label="ì§ë¬´ëª…", value=default_info.get("position_title"))
            jd_input = gr.Textbox(label="Job Description (JD)", lines=5, value=default_info.get("jd"))
            questions_input = gr.Textbox(label="ìê¸°ì†Œê°œì„œ ì§ˆë¬¸ (í•œ ì¤„ì— í•œ ê°œì”©)", lines=3, value="\n".join(default_info.get("questions", [])))
            
            with gr.Row():
                word_limit_input = gr.Number(
                    label="ìê¸°ì†Œê°œì„œ ê¸€ììˆ˜ ì œí•œ", 
                    value=300, 
                    minimum=100, 
                    maximum=1000,
                    step=50,
                    info="ìê¸°ì†Œê°œì„œ ê° ë¬¸í•­ë³„ ê¸€ììˆ˜ ì œí•œì„ ì„¤ì •í•˜ì„¸ìš”."
                )
            
            generate_guide_btn = gr.Button("ê°€ì´ë“œ ìƒì„±", variant="primary")
            guide_output = gr.Markdown(label="ìƒì„±ëœ ê°€ì´ë“œ", value=f"**ê°€ì´ë“œ:**\n{default_info.get('guide')}")
        
        with gr.TabItem("ë©´ì ‘ ëŒ€í™”", id=1):
            gr.Markdown("## ğŸ’¬ ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜")
            gr.Markdown("ë©´ì ‘ê´€ì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ê±°ë‚˜, 'AI ë‹µë³€ ìƒì„±' ë²„íŠ¼ì„ ëˆŒëŸ¬ë³´ì„¸ìš”. ë©´ì ‘ê´€ì´ íŒë‹¨í•˜ëŠ” ìê¸°ì†Œê°œì„œ ì™„ì„±ë„ê°€ 100%ê°€ ë˜ë©´ ë©´ì ‘ì´ ì¢…ë£Œë©ë‹ˆë‹¤.")
            
            with gr.Row():
                progress_display = gr.Markdown("ìê¸°ì†Œê°œì„œ ì™„ì„±ë„: 0%")
            reason_display = gr.Markdown("", visible=False)
            chatbot = gr.Chatbot(label="ë©´ì ‘ ëŒ€í™”", bubble_full_width=False, avatar_images=("ğŸ‘¤", "ğŸ‘”"), height=500)
            msg = gr.Textbox(label="ë©”ì‹œì§€ ì…ë ¥", placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...", lines=2)
            with gr.Row():
                submit_btn = gr.Button("ì „ì†¡", variant="primary")
                ai_reply_btn = gr.Button("AI ë‹µë³€ ìƒì„±", variant="secondary")
                clear_btn = gr.Button("ì´ˆê¸°í™”")

        with gr.TabItem("ìê¸°ì†Œê°œì„œ ìƒì„±", id=2):
            gr.Markdown("## ğŸ“ ìê¸°ì†Œê°œì„œ ë‹µë³€ ìƒì„±")
            gr.Markdown("ë©´ì ‘ì´ ì™„ë£Œë˜ë©´ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìê¸°ì†Œê°œì„œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            
            generate_btn = gr.Button("ìê¸°ì†Œê°œì„œ ìƒì„± ì‹œì‘", variant="primary", size="lg")
            cover_letter_progress_display = gr.Markdown("", visible=False)

            cover_letter_outputs = []
            guideline_outputs = []
            for i, question in enumerate(default_info.get('questions', [])):
                with gr.Accordion(f"ë¬¸í•­ {i+1}: {question[:50]}...", open=True):
                    gr.Markdown(f"**{question}**")
                    
                    with gr.Tabs():
                        with gr.TabItem("ìƒì„±ëœ ë‹µë³€"):
                            output = gr.Textbox(
                                label=f"ë‹µë³€ {i+1}", 
                                lines=8, 
                                max_lines=20,
                                interactive=False,
                                show_copy_button=True,
                                placeholder="ìê¸°ì†Œê°œì„œ ë‹µë³€ì´ ìƒì„±ë˜ë©´ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤...",
                                info="ê¸´ ë‹µë³€ì˜ ê²½ìš° ìŠ¤í¬ë¡¤í•˜ì—¬ ì „ì²´ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                            )
                            cover_letter_outputs.append(output)
                        
                        with gr.TabItem("ë‹µë³€ ê°€ì´ë“œë¼ì¸"):
                            guideline = gr.Markdown(value="ê°€ì´ë“œë¼ì¸ì´ ìƒì„±ë˜ë©´ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.")
                            guideline_outputs.append(guideline)

    # Event Handlers
    generate_guide_btn.click(
        fn=update_guide_and_info,
        inputs=[company_name_input, position_title_input, jd_input, questions_input, word_limit_input],
        outputs=[shared_info, guide_output]
    )
    
    submit_btn.click(user_submit, [msg, chatbot], [msg, chatbot]).then(bot_response, [chatbot, shared_info], [chatbot, progress_display, reason_display])
    msg.submit(user_submit, [msg, chatbot], [msg, chatbot]).then(bot_response, [chatbot, shared_info], [chatbot, progress_display, reason_display])
    ai_reply_btn.click(generate_ai_reply, [chatbot, shared_info], [chatbot, progress_display, reason_display])
    clear_btn.click(lambda: ([], "ìê¸°ì†Œê°œì„œ ì™„ì„±ë„: 0%", ""), None, [chatbot, progress_display, reason_display], queue=False)
    generate_btn.click(generate_all_cover_letters, [chatbot, shared_info], cover_letter_outputs + guideline_outputs + [cover_letter_progress_display])

if __name__ == "__main__":
    demo.launch(share=True)