import os
from dotenv import load_dotenv
import gradio as gr
from langchain_openai import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage
from typing import Iterator
import yaml
import json

# Load environment variables
load_dotenv()

# Initialize the ChatOpenAI model with streaming enabled
llm = ChatOpenAI(
    model_name="gpt-4",
    temperature=0.2,
    streaming=True
)

student_llm = ChatOpenAI(
    model_name="gpt-4",
    temperature=0.7,
    streaming=True
)

cover_letter_llm = ChatOpenAI(
    model_name="gpt-4",
    temperature=0.3,
    streaming=True
)

with open("prompt.yaml", "r", encoding='utf-8') as f:
    data = yaml.safe_load(f)
    interviewer_prompt = data["Interviewer"]
    student_prompt = data["Student"]

with open("example_info.json", "r", encoding='utf-8') as f:
    example_info = json.load(f)

def get_student_response(interviewer_message, history):
    # Prepare context for student LLM
    messages = [
        HumanMessage(content=student_prompt.format(**example_info)),
        HumanMessage(content="ì´ì „ ëŒ€í™” ê¸°ë¡:")
    ]
    
    # Add conversation history
    for human, ai in history:
        messages.extend([
            HumanMessage(content=f"í•™ìƒ: {human}"),
            HumanMessage(content=f"ë©´ì ‘ê´€: {ai}")
        ])
    
    # Add current interviewer message
    messages.append(HumanMessage(content=f"ë©´ì ‘ê´€: {interviewer_message}"))
    messages.append(HumanMessage(content="ìœ„ ëŒ€í™” ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ, ë©´ì ‘ê´€ì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."))
    
    # Get streaming response
    response_stream = student_llm.stream(messages)
    
    # Stream the response
    partial_message = ""
    for chunk in response_stream:
        if chunk.content:
            partial_message += chunk.content
            yield partial_message

def respond(message, history):
    # Convert history to LangChain message format
    messages = [HumanMessage(content=interviewer_prompt.format(**example_info))]
    for human, ai in history:
        messages.extend([
            HumanMessage(content=human),
            AIMessage(content=ai)
        ])
    
    # Add the current message
    messages.append(HumanMessage(content=message))
    
    # Get streaming response from LangChain
    response_stream = llm.stream(messages)
    
    # Stream the response
    partial_message = ""
    for chunk in response_stream:
        if chunk.content:
            partial_message += chunk.content
            yield partial_message

def generate_cover_letter_response(question, conversation_history):
    """Generate a cover letter response for a specific question based on conversation history"""
    
    # Prepare conversation context
    conversation_text = ""
    for human, ai in conversation_history:
        conversation_text += f"í•™ìƒ: {human}\në©´ì ‘ê´€: {ai}\n\n"
    
    prompt = f"""
ë‹¤ìŒì€ {example_info['company_name']} {example_info['position_title']} ì§ë¬´ ë©´ì ‘ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤:

{conversation_text}

ìœ„ ë©´ì ‘ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒ ìê¸°ì†Œê°œì„œ ë¬¸í•­ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

**ë¬¸í•­: {question}**

**ì‘ì„± ì§€ì¹¨:**
- ë©´ì ‘ì—ì„œ ì–¸ê¸‰ëœ êµ¬ì²´ì ì¸ ê²½í—˜ê³¼ ì‚¬ë¡€ë¥¼ í™œìš©í•˜ì„¸ìš”
- 300-500ì ì •ë„ì˜ ì ì ˆí•œ ê¸¸ì´ë¡œ ì‘ì„±í•˜ì„¸ìš”
- STAR ë°©ì‹(ìƒí™©-ê³¼ì œ-í–‰ë™-ê²°ê³¼)ì„ í™œìš©í•˜ì—¬ êµ¬ì¡°í™”í•˜ì„¸ìš”
- {example_info['company_name']}ì˜ ì¸ì¬ìƒ({example_info['core_values']})ê³¼ ì—°ê³„í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”
- ì „ë¬¸ì ì´ê³  ì§„ì •ì„± ìˆëŠ” ë¬¸ì²´ë¡œ ì‘ì„±í•˜ì„¸ìš”

ìê¸°ì†Œê°œì„œ ë‹µë³€:
"""
    
    messages = [HumanMessage(content=prompt)]
    response_stream = cover_letter_llm.stream(messages)
    
    partial_message = ""
    for chunk in response_stream:
        if chunk.content:
            partial_message += chunk.content
            yield partial_message

def check_interview_end(history):
    """Check if the interview has ended"""
    if not history:
        return False
    
    # Check the last AI response for END_OF_INTERVIEW
    last_ai_response = history[-1][1] if history[-1][1] else ""
    return "[END_OF_INTERVIEW]" in last_ai_response

with gr.Blocks(theme=gr.themes.Soft()) as demo:
    with gr.Tabs() as tabs:
        with gr.TabItem("ë©´ì ‘ ëŒ€í™”", id=0) as chat_tab:
            chatbot = gr.Chatbot(
                label="ë©´ì ‘ ëŒ€í™”",
                bubble_full_width=False,
                avatar_images=("ğŸ‘¤", "ğŸ‘”"),
                height=500
            )
            msg = gr.Textbox(
                label="ë©”ì‹œì§€ ì…ë ¥",
                placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ 'AI ë‹µë³€ ìƒì„±' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”...",
                lines=2
            )
            
            with gr.Row():
                submit = gr.Button("ì „ì†¡", variant="primary")
                ai_reply = gr.Button("AI ë‹µë³€ ìƒì„±", variant="secondary")
                clear = gr.Button("ëŒ€í™” ì´ˆê¸°í™”")
            
            with gr.Accordion("ì˜ˆì‹œ ë‹µë³€", open=False):
                examples = gr.Examples(
                    examples=["ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” ë©´ì ‘ ë³´ëŸ¬ ì™”ìŠµë‹ˆë‹¤.", 
                             "ì œ ì„±ì¥ ê³¼ì •ì— ëŒ€í•´ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                             "ì´ ì§ë¬´ë¥¼ ìœ„í•´ ì¤€ë¹„í•œ í”„ë¡œì íŠ¸ ê²½í—˜ì´ ìˆìŠµë‹ˆë‹¤."],
                    inputs=msg
                )
        
        with gr.TabItem("ìê¸°ì†Œê°œì„œ ìƒì„±", id=1) as cover_letter_tab:
            gr.Markdown("## ğŸ“ ìê¸°ì†Œê°œì„œ ë‹µë³€ ìƒì„±")
            gr.Markdown("ë©´ì ‘ì´ ì™„ë£Œë˜ë©´ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìê¸°ì†Œê°œì„œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            
            generate_btn = gr.Button("ìê¸°ì†Œê°œì„œ ìƒì„±", variant="primary", size="lg")
            
            cover_letter_outputs = []
            for i, question in enumerate(example_info.get('questions', [])):
                with gr.Accordion(f"ë¬¸í•­ {i+1}: {question[:50]}...", open=False):
                    gr.Markdown(f"**{question}**")
                    output = gr.Textbox(
                        label=f"ë‹µë³€ {i+1}",
                        lines=8,
                        placeholder="ë©´ì ‘ ì™„ë£Œ í›„ ë‹µë³€ì´ ìƒì„±ë©ë‹ˆë‹¤...",
                        interactive=False
                    )
                    cover_letter_outputs.append(output)

    def user_submit(message, history):
        if message.strip() == "":
            return "", history, gr.update()
        history = history + [[message, None]]
        return "", history, gr.update()

    def bot_response(history):
        if not history or history[-1][1] is not None:
            return history, gr.update()
        message = history[-1][0]
        history[-1][1] = ""
        for chunk in respond(message, history[:-1]):
            history[-1][1] = chunk
            yield history, gr.update()
        
        # Check if interview ended and switch to cover letter tab
        if check_interview_end(history):
            yield history, gr.update(selected=1)
        else:
            yield history, gr.update()

    def generate_ai_reply(history):
        if not history or not history[-1][1]:
            return history, gr.update()
        
        history.append(["", None])
        for chunk in get_student_response(history[-2][1], history[:-2]):
            history[-1][0] = chunk
            yield history, gr.update()
        
        # Now get interviewer's response to the generated reply
        history[-1][1] = ""
        for chunk in respond(history[-1][0], history[:-1]):
            history[-1][1] = chunk
            yield history, gr.update()
        
        # Check if interview ended and switch to cover letter tab
        if check_interview_end(history):
            yield history, gr.update(selected=1)
        else:
            yield history, gr.update()

    def generate_all_responses(history):
        """Generate responses for all cover letter questions"""
        if not history:
            return [gr.update(value="ë©´ì ‘ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")] * len(example_info.get('questions', []))
        
        results = []
        for question in example_info.get('questions', []):
            response = ""
            for chunk in generate_cover_letter_response(question, history):
                response = chunk
            results.append(gr.update(value=response))
        
        return results

    msg.submit(user_submit, [msg, chatbot], [msg, chatbot, tabs]).then(
        bot_response, chatbot, [chatbot, tabs]
    )
    
    submit.click(user_submit, [msg, chatbot], [msg, chatbot, tabs]).then(
        bot_response, chatbot, [chatbot, tabs]
    )
    
    ai_reply.click(generate_ai_reply, chatbot, [chatbot, tabs])
    clear.click(lambda: ([], gr.update(selected=0)), None, [chatbot, tabs], queue=False)
    
    generate_btn.click(
        generate_all_responses, 
        chatbot, 
        cover_letter_outputs
    )

if __name__ == "__main__":
    demo.launch(share=True)